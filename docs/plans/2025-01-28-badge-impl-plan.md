# E-Badge System å®ç°è®¡åˆ’

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** æ„å»ºå®Œæ•´çš„ä¼šå‘˜å¾½ç« ç®¡ç†ç³»ç»Ÿï¼ŒåŒ…å« 6 ä¸ª Rust åç«¯æœåŠ¡ã€1 ä¸ª React ç®¡ç†å‰ç«¯ã€ä»¥åŠå®Œæ•´çš„æ¨¡æ‹Ÿå¤–éƒ¨ç³»ç»Ÿã€‚

**Architecture:** é‡‡ç”¨äº‹ä»¶é©±åŠ¨ + å¾®æœåŠ¡æ¶æ„ã€‚åç«¯ä½¿ç”¨ Rust 2024 + Axum/Tonic + SQLx + Kafkaï¼Œå‰ç«¯ä½¿ç”¨ React 18 + Ant Design Pro + React Flowã€‚æ‰€æœ‰æœåŠ¡é€šè¿‡ gRPC é€šä¿¡ï¼Œå¼‚æ­¥äº‹ä»¶é€šè¿‡ Kafka ä¼ é€’ã€‚

**Tech Stack:**
- Backend: Rust 2024, Axum 0.8, Tonic 0.12, SQLx 0.8, rdkafka, Redis
- Frontend: React 18, TypeScript, Ant Design 5, ProComponents, React Flow, Zustand
- Infrastructure: PostgreSQL, Redis, Kafka, Elasticsearch

---

## é˜¶æ®µæ¦‚è§ˆ

| é˜¶æ®µ | å†…å®¹ | é¢„è®¡ä»»åŠ¡æ•° |
|------|------|-----------|
| Phase 1 | é¡¹ç›®éª¨æ¶ä¸åŸºç¡€è®¾æ–½ | 8 |
| Phase 2 | Proto å®šä¹‰ä¸å…±äº«åº“ | 6 |
| Phase 3 | è§„åˆ™å¼•æ“æœåŠ¡ | 10 |
| Phase 4 | å¾½ç« ç®¡ç†æœåŠ¡ï¼ˆCç«¯ï¼‰ | 12 |
| Phase 5 | å¾½ç« ç®¡ç†æœåŠ¡ï¼ˆBç«¯ï¼‰ | 10 |
| Phase 6 | äº‹ä»¶å¤„ç†æœåŠ¡ | 8 |
| Phase 7 | æ¨¡æ‹Ÿå¤–éƒ¨ç³»ç»Ÿ | 8 |
| Phase 8 | ç®¡ç†åå°å‰ç«¯ | 15 |
| Phase 9 | é›†æˆæµ‹è¯•ä¸ä¼˜åŒ– | 5 |

---

## Phase 1: é¡¹ç›®éª¨æ¶ä¸åŸºç¡€è®¾æ–½

### Task 1.1: åˆ›å»º Cargo Workspace æ ¹é…ç½®

**Files:**
- Create: `Cargo.toml`
- Create: `rust-toolchain.toml`
- Create: `.cargo/config.toml`

**Step 1: åˆ›å»º Cargo.toml**

```toml
[workspace]
resolver = "2"
members = [
    "crates/proto",
    "crates/shared",
    "crates/unified-rule-engine",
    "crates/badge-management-service",
    "crates/badge-admin-service",
    "crates/event-engagement-service",
    "crates/event-transaction-service",
    "crates/notification-worker",
]

[workspace.package]
version = "0.1.0"
edition = "2024"
rust-version = "1.83"
authors = ["Badge Team"]
license = "MIT"

[workspace.dependencies]
# Async runtime
tokio = { version = "1.43", features = ["full"] }

# gRPC
tonic = "0.12"
tonic-build = "0.12"
prost = "0.13"
prost-types = "0.13"

# Database
sqlx = { version = "0.8", features = ["runtime-tokio", "postgres", "json", "chrono", "uuid", "migrate"] }

# Redis
redis = { version = "0.27", features = ["tokio-comp", "cluster-async"] }

# Kafka
rdkafka = { version = "0.37", features = ["cmake-build"] }

# Web framework (for admin service)
axum = "0.8"
axum-extra = { version = "0.10", features = ["typed-header"] }
tower = "0.5"
tower-http = { version = "0.6", features = ["cors", "trace", "compression-gzip", "timeout"] }

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Error handling
thiserror = "2.0"
anyhow = "1.0"

# Observability
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter", "json"] }
opentelemetry = "0.27"
opentelemetry-otlp = "0.27"
metrics = "0.24"
metrics-exporter-prometheus = "0.16"

# Utilities
chrono = { version = "0.4", features = ["serde"] }
uuid = { version = "1.11", features = ["v4", "v7", "serde"] }
config = "0.14"
async-trait = "0.1"
futures = "0.3"
dashmap = "6.0"
parking_lot = "0.12"

# Validation
validator = { version = "0.18", features = ["derive"] }

# Testing
mockall = "0.13"
tokio-test = "0.4"
fake = { version = "3.0", features = ["derive", "chrono", "uuid"] }
```

**Step 2: åˆ›å»º rust-toolchain.toml**

```toml
[toolchain]
channel = "stable"
components = ["rustfmt", "clippy"]
```

**Step 3: åˆ›å»º .cargo/config.toml**

```toml
[build]
rustflags = ["-C", "link-arg=-fuse-ld=lld"]

[target.x86_64-unknown-linux-gnu]
linker = "clang"
rustflags = ["-C", "link-arg=-fuse-ld=lld"]

[target.aarch64-apple-darwin]
rustflags = ["-C", "link-arg=-fuse-ld=lld"]

[alias]
t = "test"
c = "clippy"
b = "build"
r = "run"
```

**Step 4: éªŒè¯é…ç½®**

Run: `cargo --version && rustc --version`
Expected: æ˜¾ç¤º cargo å’Œ rustc ç‰ˆæœ¬

**Step 5: æäº¤**

```bash
git add Cargo.toml rust-toolchain.toml .cargo/
git commit -m "chore: åˆå§‹åŒ– Cargo workspace é…ç½®"
```

---

### Task 1.2: åˆ›å»º crates ç›®å½•ç»“æ„

**Files:**
- Create: `crates/proto/Cargo.toml`
- Create: `crates/proto/src/lib.rs`
- Create: `crates/shared/Cargo.toml`
- Create: `crates/shared/src/lib.rs`

**Step 1: åˆ›å»º proto crate**

`crates/proto/Cargo.toml`:
```toml
[package]
name = "badge-proto"
version.workspace = true
edition.workspace = true

[dependencies]
prost = { workspace = true }
prost-types = { workspace = true }
tonic = { workspace = true }
serde = { workspace = true }
chrono = { workspace = true }

[build-dependencies]
tonic-build = { workspace = true }
```

`crates/proto/src/lib.rs`:
```rust
//! gRPC/Protobuf å®šä¹‰
//!
//! æ­¤ crate åŒ…å«æ‰€æœ‰æœåŠ¡é—´é€šä¿¡çš„ protobuf å®šä¹‰å’Œç”Ÿæˆçš„ Rust ä»£ç ã€‚

pub mod badge {
    // å°†åœ¨åç»­ä»»åŠ¡ä¸­æ·»åŠ  proto ç”Ÿæˆä»£ç 
}
```

**Step 2: åˆ›å»º shared crate**

`crates/shared/Cargo.toml`:
```toml
[package]
name = "badge-shared"
version.workspace = true
edition.workspace = true

[dependencies]
tokio = { workspace = true }
sqlx = { workspace = true }
redis = { workspace = true }
rdkafka = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
thiserror = { workspace = true }
anyhow = { workspace = true }
tracing = { workspace = true }
tracing-subscriber = { workspace = true }
chrono = { workspace = true }
uuid = { workspace = true }
config = { workspace = true }
async-trait = { workspace = true }
```

`crates/shared/src/lib.rs`:
```rust
//! å…±äº«åº“
//!
//! åŒ…å«æ‰€æœ‰æœåŠ¡å…±ç”¨çš„é…ç½®ã€é”™è¯¯å¤„ç†ã€æ•°æ®åº“è¿æ¥ã€ç¼“å­˜ã€Kafka ç­‰åŸºç¡€è®¾æ–½ä»£ç ã€‚

pub mod config;
pub mod error;
pub mod database;
pub mod cache;
pub mod kafka;
pub mod telemetry;
```

**Step 3: åˆ›å»ºç©ºæ¨¡å—æ–‡ä»¶**

```bash
mkdir -p crates/shared/src
touch crates/shared/src/config.rs
touch crates/shared/src/error.rs
touch crates/shared/src/database.rs
touch crates/shared/src/cache.rs
touch crates/shared/src/kafka.rs
touch crates/shared/src/telemetry.rs
```

**Step 4: éªŒè¯ç¼–è¯‘**

Run: `cargo check -p badge-proto -p badge-shared`
Expected: ç¼–è¯‘æˆåŠŸï¼ˆå¯èƒ½æœ‰ unused è­¦å‘Šï¼‰

**Step 5: æäº¤**

```bash
git add crates/
git commit -m "chore: åˆ›å»º proto å’Œ shared crate éª¨æ¶"
```

---

### Task 1.3: åˆ›å»ºæœåŠ¡ crate éª¨æ¶

**Files:**
- Create: `crates/unified-rule-engine/Cargo.toml`
- Create: `crates/unified-rule-engine/src/main.rs`
- Create: `crates/badge-management-service/Cargo.toml`
- Create: `crates/badge-management-service/src/main.rs`
- Create: `crates/badge-admin-service/Cargo.toml`
- Create: `crates/badge-admin-service/src/main.rs`
- Create: `crates/event-engagement-service/Cargo.toml`
- Create: `crates/event-engagement-service/src/main.rs`
- Create: `crates/event-transaction-service/Cargo.toml`
- Create: `crates/event-transaction-service/src/main.rs`
- Create: `crates/notification-worker/Cargo.toml`
- Create: `crates/notification-worker/src/main.rs`

**Step 1: åˆ›å»º unified-rule-engine**

`crates/unified-rule-engine/Cargo.toml`:
```toml
[package]
name = "unified-rule-engine"
version.workspace = true
edition.workspace = true

[[bin]]
name = "rule-engine"
path = "src/main.rs"

[dependencies]
badge-proto = { path = "../proto" }
badge-shared = { path = "../shared" }
tokio = { workspace = true }
tonic = { workspace = true }
prost = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
thiserror = { workspace = true }
tracing = { workspace = true }
chrono = { workspace = true }
dashmap = { workspace = true }
parking_lot = { workspace = true }

[dev-dependencies]
mockall = { workspace = true }
tokio-test = { workspace = true }
```

`crates/unified-rule-engine/src/main.rs`:
```rust
//! ç»Ÿä¸€è§„åˆ™å¼•æ“æœåŠ¡
//!
//! æä¾›è§„åˆ™è§£æã€ç¼–è¯‘ã€æ‰§è¡Œèƒ½åŠ›ï¼Œæ”¯æŒå¤æ‚æ¡ä»¶ç»„åˆå’ŒåµŒå¥—é€»è¾‘ã€‚

use tracing::info;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    tracing_subscriber::fmt::init();
    info!("Starting unified-rule-engine...");

    // TODO: å®ç° gRPC æœåŠ¡å™¨

    Ok(())
}
```

**Step 2: åˆ›å»ºå…¶ä»–æœåŠ¡ï¼ˆæ¨¡å¼ç›¸åŒï¼‰**

ä¸ºæ¯ä¸ªæœåŠ¡åˆ›å»ºç±»ä¼¼çš„ Cargo.toml å’Œ main.rsï¼Œè°ƒæ•´ä¾èµ–å’Œæè¿°ï¼š

- `badge-management-service`: Cç«¯å¾½ç« æœåŠ¡ï¼Œä¾èµ– axum ç”¨äºå¥åº·æ£€æŸ¥
- `badge-admin-service`: Bç«¯ç®¡ç†æœåŠ¡ï¼Œä¸»è¦ä½¿ç”¨ axum
- `event-engagement-service`: è¡Œä¸ºäº‹ä»¶æœåŠ¡ï¼Œä¾èµ– rdkafka
- `event-transaction-service`: è®¢å•äº‹ä»¶æœåŠ¡ï¼Œä¾èµ– rdkafka
- `notification-worker`: é€šçŸ¥æœåŠ¡ï¼Œä¾èµ– rdkafka

**Step 3: éªŒè¯æ‰€æœ‰æœåŠ¡ç¼–è¯‘**

Run: `cargo check --workspace`
Expected: æ‰€æœ‰ crate ç¼–è¯‘æˆåŠŸ

**Step 4: æäº¤**

```bash
git add crates/
git commit -m "chore: åˆ›å»ºæ‰€æœ‰æœåŠ¡ crate éª¨æ¶"
```

---

### Task 1.4: åˆ›å»º Docker åŸºç¡€è®¾æ–½é…ç½®

**Files:**
- Create: `docker/docker-compose.infra.yml`
- Create: `docker/.env.example`

**Step 1: åˆ›å»º docker-compose.infra.yml**

```yaml
version: '3.8'

services:
  postgres:
    image: postgres:16-alpine
    container_name: badge-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-badge}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-badge_secret}
      POSTGRES_DB: ${POSTGRES_DB:-badge_db}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-badge}"]
      interval: 5s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: badge-redis
    command: redis-server --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: badge-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: badge-kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 10s
      timeout: 10s
      retries: 5

  elasticsearch:
    image: elasticsearch:8.11.0
    container_name: badge-elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - es_data:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200/_cluster/health | grep -q 'green\\|yellow'"]
      interval: 10s
      timeout: 10s
      retries: 5

volumes:
  postgres_data:
  redis_data:
  es_data:

networks:
  default:
    name: badge-network
```

**Step 2: åˆ›å»º .env.example**

```env
# PostgreSQL
POSTGRES_USER=badge
POSTGRES_PASSWORD=badge_secret
POSTGRES_DB=badge_db
DATABASE_URL=postgres://badge:badge_secret@localhost:5432/badge_db

# Redis
REDIS_URL=redis://localhost:6379

# Kafka
KAFKA_BROKERS=localhost:9092

# Elasticsearch
ELASTICSEARCH_URL=http://localhost:9200

# Service Ports
RULE_ENGINE_PORT=50051
BADGE_MANAGEMENT_PORT=50052
BADGE_ADMIN_PORT=8080
EVENT_ENGAGEMENT_PORT=50053
EVENT_TRANSACTION_PORT=50054
NOTIFICATION_WORKER_PORT=50055
```

**Step 3: éªŒè¯ Docker Compose é…ç½®**

Run: `docker compose -f docker/docker-compose.infra.yml config`
Expected: é…ç½®éªŒè¯é€šè¿‡ï¼Œæ— é”™è¯¯

**Step 4: æäº¤**

```bash
git add docker/
git commit -m "chore: æ·»åŠ åŸºç¡€è®¾æ–½ Docker Compose é…ç½®"
```

---

### Task 1.5: åˆ›å»ºæ•°æ®åº“è¿ç§»åŸºç¡€

**Files:**
- Create: `migrations/20250128_001_init_schema.sql`

**Step 1: åˆ›å»ºåˆå§‹ schema è¿ç§»**

```sql
-- å¾½ç« ç³»ç»Ÿåˆå§‹åŒ– schema
-- åŒ…å«æ ¸å¿ƒè¡¨ç»“æ„ï¼šå¾½ç« åˆ†ç±»ã€ç³»åˆ—ã€å¾½ç« ã€è§„åˆ™ã€ç”¨æˆ·å¾½ç« ã€è´¦æœ¬ç­‰

-- å¯ç”¨å¿…è¦çš„æ‰©å±•
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";

-- ==================== å¾½ç« ç»“æ„ ====================

-- ä¸€çº§åˆ†ç±»
CREATE TABLE badge_category (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(100) NOT NULL,
    description TEXT,
    sort_order INT NOT NULL DEFAULT 0,
    status VARCHAR(20) NOT NULL DEFAULT 'active', -- active, inactive
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    CONSTRAINT badge_category_name_unique UNIQUE (name)
);

-- äºŒçº§ç³»åˆ—
CREATE TABLE badge_series (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    category_id UUID NOT NULL REFERENCES badge_category(id),
    name VARCHAR(100) NOT NULL,
    description TEXT,
    sort_order INT NOT NULL DEFAULT 0,
    status VARCHAR(20) NOT NULL DEFAULT 'active',
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    CONSTRAINT badge_series_name_unique UNIQUE (category_id, name)
);

-- å¾½ç« å®šä¹‰
CREATE TABLE badge (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    series_id UUID NOT NULL REFERENCES badge_series(id),
    code VARCHAR(50) NOT NULL UNIQUE, -- ä¸šåŠ¡å”¯ä¸€æ ‡è¯†
    name VARCHAR(100) NOT NULL,
    description TEXT,
    badge_type VARCHAR(50) NOT NULL, -- transaction, engagement, identity, seasonal

    -- ç´ æ
    icon_url TEXT,
    icon_3d_url TEXT,

    -- è·å–é…ç½®
    acquire_time_start TIMESTAMPTZ,
    acquire_time_end TIMESTAMPTZ,
    max_acquire_count INT, -- NULL è¡¨ç¤ºæ— é™

    -- æŒæœ‰æœ‰æ•ˆæœŸé…ç½®
    validity_type VARCHAR(20) NOT NULL DEFAULT 'permanent', -- fixed, flexible, permanent
    validity_fixed_date TIMESTAMPTZ, -- validity_type = fixed æ—¶ä½¿ç”¨
    validity_days INT, -- validity_type = flexible æ—¶ä½¿ç”¨

    -- å‘æ”¾å¯¹è±¡
    grant_target VARCHAR(20) NOT NULL DEFAULT 'account', -- account, actual_user

    status VARCHAR(20) NOT NULL DEFAULT 'draft', -- draft, active, inactive
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_badge_series ON badge(series_id);
CREATE INDEX idx_badge_type ON badge(badge_type);
CREATE INDEX idx_badge_status ON badge(status);

-- ==================== è§„åˆ™é…ç½® ====================

-- å¾½ç« è·å–è§„åˆ™
CREATE TABLE badge_rule (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    badge_id UUID NOT NULL REFERENCES badge(id),
    name VARCHAR(100) NOT NULL,
    description TEXT,
    rule_json JSONB NOT NULL, -- è§„åˆ™ JSON
    priority INT NOT NULL DEFAULT 0, -- ä¼˜å…ˆçº§ï¼Œæ•°å€¼è¶Šå¤§è¶Šä¼˜å…ˆ
    status VARCHAR(20) NOT NULL DEFAULT 'active',
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_badge_rule_badge ON badge_rule(badge_id);
CREATE INDEX idx_badge_rule_json ON badge_rule USING GIN(rule_json);

-- ==================== ç”¨æˆ·å¾½ç«  ====================

-- ç”¨æˆ·å¾½ç« æŒæœ‰
CREATE TABLE user_badge (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id VARCHAR(100) NOT NULL, -- SWID
    badge_id UUID NOT NULL REFERENCES badge(id),
    quantity INT NOT NULL DEFAULT 1,
    status VARCHAR(20) NOT NULL DEFAULT 'active', -- active, expired, revoked, redeemed
    acquired_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    expires_at TIMESTAMPTZ,

    -- å‘æ”¾æ¥æº
    source_type VARCHAR(20) NOT NULL, -- event, scheduled, manual
    source_ref VARCHAR(200), -- æ¥æºå¼•ç”¨ï¼ˆäº‹ä»¶IDã€ä»»åŠ¡IDç­‰ï¼‰

    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_user_badge_user ON user_badge(user_id);
CREATE INDEX idx_user_badge_badge ON user_badge(badge_id);
CREATE INDEX idx_user_badge_status ON user_badge(status);
CREATE INDEX idx_user_badge_user_status ON user_badge(user_id, status);

-- å¾½ç« è´¦æœ¬ï¼ˆæµæ°´ï¼‰
CREATE TABLE badge_ledger (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id VARCHAR(100) NOT NULL,
    badge_id UUID NOT NULL REFERENCES badge(id),
    user_badge_id UUID REFERENCES user_badge(id),

    change_type VARCHAR(20) NOT NULL, -- acquire, expire, cancel, redeem_out, redeem_fail
    quantity INT NOT NULL, -- æ­£æ•°å¢åŠ ï¼Œè´Ÿæ•°å‡å°‘
    balance_after INT NOT NULL, -- å˜æ›´åä½™é¢

    -- å…³è”æ¥æº
    ref_type VARCHAR(20) NOT NULL, -- event, scheduled, manual, redemption, system
    ref_id VARCHAR(200),

    reason TEXT,
    operator VARCHAR(100), -- æ“ä½œäººï¼ˆæ‰‹åŠ¨æ“ä½œæ—¶ï¼‰

    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_badge_ledger_user ON badge_ledger(user_id);
CREATE INDEX idx_badge_ledger_badge ON badge_ledger(badge_id);
CREATE INDEX idx_badge_ledger_ref ON badge_ledger(ref_type, ref_id);
CREATE INDEX idx_badge_ledger_time ON badge_ledger(created_at);

-- ==================== å…‘æ¢ç›¸å…³ ====================

-- æƒç›Šå®šä¹‰
CREATE TABLE benefit (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    code VARCHAR(50) NOT NULL UNIQUE,
    name VARCHAR(100) NOT NULL,
    description TEXT,
    benefit_type VARCHAR(50) NOT NULL, -- digital_asset, coupon, reservation

    -- å¤–éƒ¨ç³»ç»Ÿå…³è”
    external_id VARCHAR(200),
    external_system VARCHAR(50),

    -- åº“å­˜
    total_stock INT,
    remaining_stock INT,

    status VARCHAR(20) NOT NULL DEFAULT 'active',
    config JSONB, -- æƒç›Šç‰¹å®šé…ç½®

    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- å…‘æ¢è§„åˆ™
CREATE TABLE badge_redemption_rule (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(100) NOT NULL,
    description TEXT,
    benefit_id UUID NOT NULL REFERENCES benefit(id),

    -- æ‰€éœ€å¾½ç« é…ç½®
    required_badges JSONB NOT NULL, -- [{badge_id, quantity}]

    -- å…‘æ¢æ—¶é—´é™åˆ¶
    redeem_time_start TIMESTAMPTZ,
    redeem_time_end TIMESTAMPTZ,
    redeem_after_acquire_days INT, -- è·å–åNå¤©å†…å¯å…‘æ¢

    -- å…‘æ¢é¢‘æ¬¡é™åˆ¶
    frequency_type VARCHAR(20), -- daily, weekly, monthly, yearly, account
    frequency_limit INT,

    -- è‡ªåŠ¨å…‘æ¢
    auto_redeem BOOLEAN NOT NULL DEFAULT FALSE,

    status VARCHAR(20) NOT NULL DEFAULT 'active',
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_redemption_rule_benefit ON badge_redemption_rule(benefit_id);

-- å…‘æ¢è®¢å•
CREATE TABLE redemption_order (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id VARCHAR(100) NOT NULL,
    redemption_rule_id UUID NOT NULL REFERENCES badge_redemption_rule(id),
    benefit_id UUID NOT NULL REFERENCES benefit(id),

    status VARCHAR(20) NOT NULL DEFAULT 'pending', -- pending, completed, failed, cancelled

    -- æƒç›Šå‘æ”¾ç»“æœ
    benefit_grant_ref VARCHAR(200), -- å¤–éƒ¨ç³»ç»Ÿæƒç›Šå‘æ”¾ID
    benefit_grant_at TIMESTAMPTZ,

    failure_reason TEXT,

    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_redemption_order_user ON redemption_order(user_id);
CREATE INDEX idx_redemption_order_status ON redemption_order(status);

-- å…‘æ¢æ˜ç»†
CREATE TABLE redemption_detail (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    order_id UUID NOT NULL REFERENCES redemption_order(id),
    user_badge_id UUID NOT NULL REFERENCES user_badge(id),
    badge_id UUID NOT NULL REFERENCES badge(id),
    quantity INT NOT NULL,

    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_redemption_detail_order ON redemption_detail(order_id);

-- ==================== é€šçŸ¥ç›¸å…³ ====================

-- é€šçŸ¥é…ç½®
CREATE TABLE notification_config (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    badge_id UUID REFERENCES badge(id),
    benefit_id UUID REFERENCES benefit(id),

    trigger_type VARCHAR(20) NOT NULL, -- grant, revoke, expire, expire_remind, redeem
    channels JSONB NOT NULL, -- ["app_push", "sms", "wechat", "email"]
    template_id VARCHAR(100),
    advance_days INT, -- æå‰é€šçŸ¥å¤©æ•°ï¼ˆè¿‡æœŸæé†’ï¼‰

    retry_count INT NOT NULL DEFAULT 3,
    retry_interval_seconds INT NOT NULL DEFAULT 60,

    status VARCHAR(20) NOT NULL DEFAULT 'active',
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- é€šçŸ¥ä»»åŠ¡
CREATE TABLE notification_task (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id VARCHAR(100) NOT NULL,

    trigger_type VARCHAR(20) NOT NULL,
    channels JSONB NOT NULL,
    template_id VARCHAR(100),
    template_params JSONB,

    status VARCHAR(20) NOT NULL DEFAULT 'pending', -- pending, processing, completed, failed
    retry_count INT NOT NULL DEFAULT 0,
    max_retries INT NOT NULL DEFAULT 3,

    last_error TEXT,
    completed_at TIMESTAMPTZ,

    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_notification_task_status ON notification_task(status);
CREATE INDEX idx_notification_task_user ON notification_task(user_id);

-- ==================== ç³»ç»Ÿç®¡ç† ====================

-- æ“ä½œæ—¥å¿—
CREATE TABLE operation_log (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    operator_id VARCHAR(100) NOT NULL,
    operator_name VARCHAR(100),

    module VARCHAR(50) NOT NULL,
    action VARCHAR(50) NOT NULL,
    target_type VARCHAR(50),
    target_id VARCHAR(200),

    before_data JSONB,
    after_data JSONB,

    ip_address VARCHAR(50),
    user_agent TEXT,

    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_operation_log_operator ON operation_log(operator_id);
CREATE INDEX idx_operation_log_module ON operation_log(module);
CREATE INDEX idx_operation_log_time ON operation_log(created_at);

-- æ‰¹é‡ä»»åŠ¡
CREATE TABLE batch_task (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    task_type VARCHAR(50) NOT NULL, -- batch_grant, batch_revoke, data_export

    file_url TEXT, -- ä¸Šä¼ çš„æ–‡ä»¶åœ°å€
    total_count INT NOT NULL DEFAULT 0,
    success_count INT NOT NULL DEFAULT 0,
    failure_count INT NOT NULL DEFAULT 0,

    status VARCHAR(20) NOT NULL DEFAULT 'pending', -- pending, processing, completed, failed
    progress INT NOT NULL DEFAULT 0, -- 0-100

    result_file_url TEXT, -- ç»“æœæ–‡ä»¶åœ°å€
    error_message TEXT,

    created_by VARCHAR(100) NOT NULL,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_batch_task_status ON batch_task(status);
CREATE INDEX idx_batch_task_creator ON batch_task(created_by);

-- ==================== è§¦å‘å™¨ ====================

-- æ›´æ–° updated_at è§¦å‘å™¨å‡½æ•°
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

-- ä¸ºæ‰€æœ‰è¡¨æ·»åŠ  updated_at è§¦å‘å™¨
CREATE TRIGGER update_badge_category_updated_at BEFORE UPDATE ON badge_category FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
CREATE TRIGGER update_badge_series_updated_at BEFORE UPDATE ON badge_series FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
CREATE TRIGGER update_badge_updated_at BEFORE UPDATE ON badge FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
CREATE TRIGGER update_badge_rule_updated_at BEFORE UPDATE ON badge_rule FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
CREATE TRIGGER update_user_badge_updated_at BEFORE UPDATE ON user_badge FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
CREATE TRIGGER update_benefit_updated_at BEFORE UPDATE ON benefit FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
CREATE TRIGGER update_badge_redemption_rule_updated_at BEFORE UPDATE ON badge_redemption_rule FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
CREATE TRIGGER update_redemption_order_updated_at BEFORE UPDATE ON redemption_order FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
CREATE TRIGGER update_notification_config_updated_at BEFORE UPDATE ON notification_config FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
CREATE TRIGGER update_notification_task_updated_at BEFORE UPDATE ON notification_task FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
CREATE TRIGGER update_batch_task_updated_at BEFORE UPDATE ON batch_task FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
```

**Step 2: éªŒè¯ SQL è¯­æ³•**

å¯åŠ¨ PostgreSQL å¹¶æ‰§è¡Œè¿ç§»ï¼ˆéœ€è¦å…ˆå¯åŠ¨ dockerï¼‰ï¼š

Run: `docker compose -f docker/docker-compose.infra.yml up -d postgres && sleep 5`

Run: `docker exec -i badge-postgres psql -U badge -d badge_db < migrations/20250128_001_init_schema.sql`
Expected: æ‰€æœ‰è¡¨åˆ›å»ºæˆåŠŸ

**Step 3: æäº¤**

```bash
git add migrations/
git commit -m "feat: æ·»åŠ æ•°æ®åº“åˆå§‹åŒ–è¿ç§»è„šæœ¬"
```

---

### Task 1.6: åˆ›å»ºå‰ç«¯é¡¹ç›®éª¨æ¶

**Files:**
- Create: `web/admin-ui/package.json`
- Create: `web/admin-ui/tsconfig.json`
- Create: `web/admin-ui/vite.config.ts`
- Create: `web/admin-ui/src/main.tsx`
- Create: `web/admin-ui/src/App.tsx`
- Create: `web/admin-ui/index.html`

**Step 1: åˆ›å»º package.json**

```json
{
  "name": "badge-admin-ui",
  "private": true,
  "version": "0.1.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "tsc && vite build",
    "preview": "vite preview",
    "lint": "eslint . --ext ts,tsx --report-unused-disable-directives --max-warnings 0",
    "format": "prettier --write \"src/**/*.{ts,tsx,css}\""
  },
  "dependencies": {
    "react": "^18.3.1",
    "react-dom": "^18.3.1",
    "react-router-dom": "^7.1.0",
    "antd": "^5.23.0",
    "@ant-design/pro-components": "^2.8.0",
    "@ant-design/icons": "^5.6.0",
    "zustand": "^5.0.0",
    "@tanstack/react-query": "^5.62.0",
    "axios": "^1.7.0",
    "@xyflow/react": "^12.4.0",
    "echarts": "^5.5.0",
    "echarts-for-react": "^3.0.0",
    "dayjs": "^1.11.0",
    "ahooks": "^3.8.0",
    "lodash-es": "^4.17.21"
  },
  "devDependencies": {
    "@types/react": "^18.3.0",
    "@types/react-dom": "^18.3.0",
    "@types/lodash-es": "^4.17.12",
    "@vitejs/plugin-react": "^4.3.0",
    "typescript": "^5.7.0",
    "vite": "^6.0.0",
    "eslint": "^9.17.0",
    "@eslint/js": "^9.17.0",
    "eslint-plugin-react-hooks": "^5.0.0",
    "eslint-plugin-react-refresh": "^0.4.0",
    "prettier": "^3.4.0",
    "typescript-eslint": "^8.0.0"
  }
}
```

**Step 2: åˆ›å»º tsconfig.json**

```json
{
  "compilerOptions": {
    "target": "ES2020",
    "useDefineForClassFields": true,
    "lib": ["ES2020", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "skipLibCheck": true,
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "isolatedModules": true,
    "moduleDetection": "force",
    "noEmit": true,
    "jsx": "react-jsx",
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noFallthroughCasesInSwitch": true,
    "baseUrl": ".",
    "paths": {
      "@/*": ["src/*"]
    }
  },
  "include": ["src"],
  "references": [{ "path": "./tsconfig.node.json" }]
}
```

**Step 3: åˆ›å»º vite.config.ts**

```typescript
import { defineConfig } from 'vite';
import react from '@vitejs/plugin-react';
import { resolve } from 'path';

export default defineConfig({
  plugins: [react()],
  resolve: {
    alias: {
      '@': resolve(__dirname, 'src'),
    },
  },
  server: {
    port: 3000,
    proxy: {
      '/api': {
        target: 'http://localhost:8080',
        changeOrigin: true,
      },
    },
  },
});
```

**Step 4: åˆ›å»ºå…¥å£æ–‡ä»¶**

`web/admin-ui/index.html`:
```html
<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>å¾½ç« ç®¡ç†ç³»ç»Ÿ</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>
```

`web/admin-ui/src/main.tsx`:
```tsx
import React from 'react';
import ReactDOM from 'react-dom/client';
import { ConfigProvider } from 'antd';
import zhCN from 'antd/locale/zh_CN';
import App from './App';
import 'antd/dist/reset.css';

ReactDOM.createRoot(document.getElementById('root')!).render(
  <React.StrictMode>
    <ConfigProvider locale={zhCN}>
      <App />
    </ConfigProvider>
  </React.StrictMode>,
);
```

`web/admin-ui/src/App.tsx`:
```tsx
import { BrowserRouter } from 'react-router-dom';
import { QueryClient, QueryClientProvider } from '@tanstack/react-query';

const queryClient = new QueryClient({
  defaultOptions: {
    queries: {
      staleTime: 5 * 60 * 1000,
      retry: 1,
    },
  },
});

function App() {
  return (
    <QueryClientProvider client={queryClient}>
      <BrowserRouter>
        <div>
          <h1>å¾½ç« ç®¡ç†ç³»ç»Ÿ</h1>
          <p>ç³»ç»Ÿåˆå§‹åŒ–ä¸­...</p>
        </div>
      </BrowserRouter>
    </QueryClientProvider>
  );
}

export default App;
```

**Step 5: å®‰è£…ä¾èµ–å¹¶éªŒè¯**

Run: `cd web/admin-ui && pnpm install && pnpm run build`
Expected: æ„å»ºæˆåŠŸ

**Step 6: æäº¤**

```bash
git add web/
git commit -m "chore: åˆ›å»ºå‰ç«¯é¡¹ç›®éª¨æ¶"
```

---

### Task 1.7: åˆ›å»ºå¼€å‘è„šæœ¬

**Files:**
- Create: `scripts/dev-setup.sh`
- Create: `scripts/run-tests.sh`
- Create: `Makefile`

**Step 1: åˆ›å»º dev-setup.sh**

```bash
#!/bin/bash
set -e

echo "ğŸš€ Setting up development environment..."

# æ£€æŸ¥ä¾èµ–
command -v docker >/dev/null 2>&1 || { echo "âŒ Docker is required but not installed."; exit 1; }
command -v cargo >/dev/null 2>&1 || { echo "âŒ Cargo is required but not installed."; exit 1; }
command -v pnpm >/dev/null 2>&1 || { echo "âŒ pnpm is required but not installed."; exit 1; }

# å¯åŠ¨åŸºç¡€è®¾æ–½
echo "ğŸ“¦ Starting infrastructure..."
docker compose -f docker/docker-compose.infra.yml up -d

# ç­‰å¾…æœåŠ¡å°±ç»ª
echo "â³ Waiting for services to be ready..."
sleep 10

# è¿è¡Œæ•°æ®åº“è¿ç§»
echo "ğŸ—ƒï¸ Running database migrations..."
docker exec -i badge-postgres psql -U badge -d badge_db < migrations/20250128_001_init_schema.sql || true

# å®‰è£…å‰ç«¯ä¾èµ–
echo "ğŸ“¦ Installing frontend dependencies..."
cd web/admin-ui && pnpm install && cd ../..

# æ„å»º Rust é¡¹ç›®
echo "ğŸ”¨ Building Rust project..."
cargo build

echo "âœ… Development environment is ready!"
echo ""
echo "Available commands:"
echo "  make dev-backend   - Start all backend services"
echo "  make dev-frontend  - Start frontend dev server"
echo "  make test          - Run all tests"
echo "  make infra-up      - Start infrastructure"
echo "  make infra-down    - Stop infrastructure"
```

**Step 2: åˆ›å»º run-tests.sh**

```bash
#!/bin/bash
set -e

echo "ğŸ§ª Running tests..."

# Rust æµ‹è¯•
echo "ğŸ“¦ Running Rust tests..."
cargo test --workspace

# å‰ç«¯æµ‹è¯•
echo "ğŸ“¦ Running frontend tests..."
cd web/admin-ui && pnpm run lint && cd ../..

echo "âœ… All tests passed!"
```

**Step 3: åˆ›å»º Makefile**

```makefile
.PHONY: all setup build test clean dev-backend dev-frontend infra-up infra-down

# é»˜è®¤ç›®æ ‡
all: build

# å¼€å‘ç¯å¢ƒè®¾ç½®
setup:
	./scripts/dev-setup.sh

# æ„å»º
build:
	cargo build --workspace
	cd web/admin-ui && pnpm run build

# æµ‹è¯•
test:
	./scripts/run-tests.sh

# æ¸…ç†
clean:
	cargo clean
	rm -rf web/admin-ui/dist
	rm -rf web/admin-ui/node_modules

# å¯åŠ¨åç«¯å¼€å‘æœåŠ¡
dev-backend:
	cargo run --bin rule-engine &
	cargo run --bin badge-management &
	cargo run --bin badge-admin &

# å¯åŠ¨å‰ç«¯å¼€å‘æœåŠ¡
dev-frontend:
	cd web/admin-ui && pnpm run dev

# åŸºç¡€è®¾æ–½ç®¡ç†
infra-up:
	docker compose -f docker/docker-compose.infra.yml up -d

infra-down:
	docker compose -f docker/docker-compose.infra.yml down

infra-logs:
	docker compose -f docker/docker-compose.infra.yml logs -f

# æ•°æ®åº“è¿ç§»
db-migrate:
	docker exec -i badge-postgres psql -U badge -d badge_db < migrations/20250128_001_init_schema.sql

# ä»£ç æ£€æŸ¥
lint:
	cargo clippy --workspace -- -D warnings
	cd web/admin-ui && pnpm run lint

# æ ¼å¼åŒ–
fmt:
	cargo fmt --all
	cd web/admin-ui && pnpm run format
```

**Step 4: è®¾ç½®æ‰§è¡Œæƒé™**

Run: `chmod +x scripts/*.sh`

**Step 5: éªŒè¯**

Run: `make --version && make build`
Expected: Make ç‰ˆæœ¬æ˜¾ç¤ºï¼Œæ„å»ºæˆåŠŸ

**Step 6: æäº¤**

```bash
git add scripts/ Makefile
git commit -m "chore: æ·»åŠ å¼€å‘è„šæœ¬å’Œ Makefile"
```

---

### Task 1.8: å®Œæˆ Phase 1 éªŒè¯

**Step 1: è¿è¡Œå®Œæ•´æ„å»º**

Run: `cargo build --workspace`
Expected: æ‰€æœ‰ crate æ„å»ºæˆåŠŸ

**Step 2: è¿è¡Œ lint**

Run: `cargo clippy --workspace -- -D warnings`
Expected: æ— è­¦å‘Šï¼ˆæˆ–åªæœ‰é¢„æœŸçš„ unused è­¦å‘Šï¼‰

**Step 3: æ£€æŸ¥é¡¹ç›®ç»“æ„**

Run: `find . -type f -name "*.rs" | head -20`
Expected: æ˜¾ç¤ºæ‰€æœ‰ Rust æºæ–‡ä»¶

**Step 4: æäº¤ Phase 1 å®Œæˆæ ‡è®°**

```bash
git add -A
git commit -m "milestone: å®Œæˆ Phase 1 - é¡¹ç›®éª¨æ¶ä¸åŸºç¡€è®¾æ–½"
```

---

## Phase 2: Proto å®šä¹‰ä¸å…±äº«åº“

### Task 2.1: å®šä¹‰è§„åˆ™å¼•æ“ Proto

**Files:**
- Create: `crates/proto/src/rule_engine.proto`
- Modify: `crates/proto/build.rs`

**Step 1: åˆ›å»º rule_engine.proto**

```protobuf
syntax = "proto3";

package badge.rule_engine;

import "google/protobuf/struct.proto";
import "google/protobuf/timestamp.proto";

// è§„åˆ™å¼•æ“æœåŠ¡
service RuleEngineService {
  // è¯„ä¼°è§„åˆ™
  rpc Evaluate(EvaluateRequest) returns (EvaluateResponse);

  // æ‰¹é‡è¯„ä¼°è§„åˆ™
  rpc BatchEvaluate(BatchEvaluateRequest) returns (BatchEvaluateResponse);

  // åŠ è½½/æ›´æ–°è§„åˆ™
  rpc LoadRule(LoadRuleRequest) returns (LoadRuleResponse);

  // åˆ é™¤è§„åˆ™
  rpc DeleteRule(DeleteRuleRequest) returns (DeleteRuleResponse);

  // æµ‹è¯•è§„åˆ™
  rpc TestRule(TestRuleRequest) returns (TestRuleResponse);
}

// è§„åˆ™å®šä¹‰
message Rule {
  string id = 1;
  string name = 2;
  string version = 3;
  RuleNode root = 4;
  google.protobuf.Timestamp created_at = 5;
  google.protobuf.Timestamp updated_at = 6;
}

// è§„åˆ™èŠ‚ç‚¹ï¼ˆæ¡ä»¶æˆ–ç»„ï¼‰
message RuleNode {
  oneof node {
    ConditionNode condition = 1;
    GroupNode group = 2;
  }
}

// æ¡ä»¶èŠ‚ç‚¹
message ConditionNode {
  string field = 1;
  Operator operator = 2;
  google.protobuf.Value value = 3;
}

// ç»„èŠ‚ç‚¹
message GroupNode {
  LogicalOperator operator = 1;
  repeated RuleNode children = 2;
}

// æ“ä½œç¬¦
enum Operator {
  OPERATOR_UNSPECIFIED = 0;
  EQ = 1;
  NEQ = 2;
  GT = 3;
  GTE = 4;
  LT = 5;
  LTE = 6;
  BETWEEN = 7;
  IN = 8;
  NOT_IN = 9;
  CONTAINS = 10;
  STARTS_WITH = 11;
  ENDS_WITH = 12;
  REGEX = 13;
  IS_EMPTY = 14;
  IS_NOT_EMPTY = 15;
  CONTAINS_ANY = 16;
  CONTAINS_ALL = 17;
  BEFORE = 18;
  AFTER = 19;
}

// é€»è¾‘æ“ä½œç¬¦
enum LogicalOperator {
  LOGICAL_OPERATOR_UNSPECIFIED = 0;
  AND = 1;
  OR = 2;
}

// è¯„ä¼°è¯·æ±‚
message EvaluateRequest {
  string rule_id = 1;
  google.protobuf.Struct context = 2; // ä¸Šä¸‹æ–‡æ•°æ®
}

// è¯„ä¼°å“åº”
message EvaluateResponse {
  bool matched = 1;
  string rule_id = 2;
  string rule_name = 3;
  repeated string matched_conditions = 4; // åŒ¹é…çš„æ¡ä»¶è·¯å¾„
  int64 evaluation_time_ms = 5;
}

// æ‰¹é‡è¯„ä¼°è¯·æ±‚
message BatchEvaluateRequest {
  repeated string rule_ids = 1;
  google.protobuf.Struct context = 2;
}

// æ‰¹é‡è¯„ä¼°å“åº”
message BatchEvaluateResponse {
  repeated EvaluateResponse results = 1;
  int64 total_evaluation_time_ms = 2;
}

// åŠ è½½è§„åˆ™è¯·æ±‚
message LoadRuleRequest {
  Rule rule = 1;
}

// åŠ è½½è§„åˆ™å“åº”
message LoadRuleResponse {
  bool success = 1;
  string message = 2;
}

// åˆ é™¤è§„åˆ™è¯·æ±‚
message DeleteRuleRequest {
  string rule_id = 1;
}

// åˆ é™¤è§„åˆ™å“åº”
message DeleteRuleResponse {
  bool success = 1;
  string message = 2;
}

// æµ‹è¯•è§„åˆ™è¯·æ±‚
message TestRuleRequest {
  Rule rule = 1; // å¾…æµ‹è¯•çš„è§„åˆ™ï¼ˆä¸éœ€è¦å…ˆåŠ è½½ï¼‰
  google.protobuf.Struct context = 2;
}

// æµ‹è¯•è§„åˆ™å“åº”
message TestRuleResponse {
  bool matched = 1;
  repeated string matched_conditions = 2;
  repeated string evaluation_trace = 3; // è¯„ä¼°è¿‡ç¨‹è¿½è¸ª
  int64 evaluation_time_ms = 4;
}
```

**Step 2: åˆ›å»º build.rs**

```rust
fn main() -> Result<(), Box<dyn std::error::Error>> {
    tonic_build::configure()
        .build_server(true)
        .build_client(true)
        .out_dir("src/generated")
        .compile_protos(
            &["src/rule_engine.proto"],
            &["src/"],
        )?;
    Ok(())
}
```

**Step 3: åˆ›å»º generated ç›®å½•å¹¶æ›´æ–° lib.rs**

```bash
mkdir -p crates/proto/src/generated
```

`crates/proto/src/lib.rs`:
```rust
//! gRPC/Protobuf å®šä¹‰

pub mod rule_engine {
    include!("generated/badge.rule_engine.rs");
}
```

**Step 4: ç¼–è¯‘éªŒè¯**

Run: `cargo build -p badge-proto`
Expected: proto ç¼–è¯‘æˆåŠŸï¼Œç”Ÿæˆ Rust ä»£ç 

**Step 5: æäº¤**

```bash
git add crates/proto/
git commit -m "feat(proto): æ·»åŠ è§„åˆ™å¼•æ“ proto å®šä¹‰"
```

---

### Task 2.2: å®šä¹‰å¾½ç« æœåŠ¡ Proto

**Files:**
- Create: `crates/proto/src/badge.proto`
- Modify: `crates/proto/build.rs`

**Step 1: åˆ›å»º badge.proto**

```protobuf
syntax = "proto3";

package badge.management;

import "google/protobuf/timestamp.proto";
import "google/protobuf/wrappers.proto";

// å¾½ç« ç®¡ç†æœåŠ¡ï¼ˆCç«¯ï¼‰
service BadgeManagementService {
  // è·å–ç”¨æˆ·å¾½ç« åˆ—è¡¨
  rpc GetUserBadges(GetUserBadgesRequest) returns (GetUserBadgesResponse);

  // è·å–å¾½ç« è¯¦æƒ…
  rpc GetBadgeDetail(GetBadgeDetailRequest) returns (GetBadgeDetailResponse);

  // è·å–å¾½ç« å¢™
  rpc GetBadgeWall(GetBadgeWallRequest) returns (GetBadgeWallResponse);

  // å‘æ”¾å¾½ç« ï¼ˆå†…éƒ¨è°ƒç”¨ï¼‰
  rpc GrantBadge(GrantBadgeRequest) returns (GrantBadgeResponse);

  // å–æ¶ˆå¾½ç« ï¼ˆå†…éƒ¨è°ƒç”¨ï¼‰
  rpc RevokeBadge(RevokeBadgeRequest) returns (RevokeBadgeResponse);

  // å…‘æ¢å¾½ç« 
  rpc RedeemBadge(RedeemBadgeRequest) returns (RedeemBadgeResponse);

  // ç½®é¡¶/ä½©æˆ´å¾½ç« 
  rpc PinBadge(PinBadgeRequest) returns (PinBadgeResponse);
}

// å¾½ç« çŠ¶æ€
enum BadgeStatus {
  BADGE_STATUS_UNSPECIFIED = 0;
  ACTIVE = 1;
  EXPIRED = 2;
  REVOKED = 3;
  REDEEMED = 4;
}

// å¾½ç« ç±»å‹
enum BadgeType {
  BADGE_TYPE_UNSPECIFIED = 0;
  TRANSACTION = 1;
  ENGAGEMENT = 2;
  IDENTITY = 3;
  SEASONAL = 4;
}

// å¾½ç« ä¿¡æ¯
message Badge {
  string id = 1;
  string code = 2;
  string name = 3;
  string description = 4;
  BadgeType badge_type = 5;
  string category_name = 6;
  string series_name = 7;
  string icon_url = 8;
  string icon_3d_url = 9;
}

// ç”¨æˆ·å¾½ç« ä¿¡æ¯
message UserBadge {
  string id = 1;
  Badge badge = 2;
  int32 quantity = 3;
  BadgeStatus status = 4;
  google.protobuf.Timestamp acquired_at = 5;
  google.protobuf.Timestamp expires_at = 6;
  bool is_pinned = 7;
}

// è·å–ç”¨æˆ·å¾½ç« åˆ—è¡¨è¯·æ±‚
message GetUserBadgesRequest {
  string user_id = 1;
  google.protobuf.StringValue badge_type = 2;
  google.protobuf.StringValue status = 3;
  int32 page = 4;
  int32 page_size = 5;
}

// è·å–ç”¨æˆ·å¾½ç« åˆ—è¡¨å“åº”
message GetUserBadgesResponse {
  repeated UserBadge badges = 1;
  int32 total = 2;
  int32 page = 3;
  int32 page_size = 4;
}

// è·å–å¾½ç« è¯¦æƒ…è¯·æ±‚
message GetBadgeDetailRequest {
  string badge_id = 1;
  string user_id = 2; // å¯é€‰ï¼Œç”¨äºè·å–ç”¨æˆ·æŒæœ‰çŠ¶æ€
}

// è·å–å¾½ç« è¯¦æƒ…å“åº”
message GetBadgeDetailResponse {
  Badge badge = 1;
  google.protobuf.Int32Value user_quantity = 2;
  google.protobuf.Timestamp user_acquired_at = 3;
  google.protobuf.Timestamp user_expires_at = 4;
  bool can_redeem = 5;
}

// è·å–å¾½ç« å¢™è¯·æ±‚
message GetBadgeWallRequest {
  string user_id = 1;
  string sort_by = 2; // name, type, acquired_at
  string sort_order = 3; // asc, desc
  repeated string badge_types = 4; // ç­›é€‰ç±»å‹
}

// è·å–å¾½ç« å¢™å“åº”
message GetBadgeWallResponse {
  repeated UserBadge badges = 1;
  int32 total_count = 2;
  int32 active_count = 3;
  int32 expired_count = 4;
  int32 redeemed_count = 5;
}

// å‘æ”¾å¾½ç« è¯·æ±‚
message GrantBadgeRequest {
  string user_id = 1;
  string badge_id = 2;
  int32 quantity = 3;
  string source_type = 4; // event, scheduled, manual
  string source_ref = 5;
  string operator = 6; // æ‰‹åŠ¨å‘æ”¾æ—¶çš„æ“ä½œäºº
}

// å‘æ”¾å¾½ç« å“åº”
message GrantBadgeResponse {
  bool success = 1;
  string user_badge_id = 2;
  string message = 3;
}

// å–æ¶ˆå¾½ç« è¯·æ±‚
message RevokeBadgeRequest {
  string user_id = 1;
  string badge_id = 2;
  int32 quantity = 3;
  string reason = 4;
  string operator = 5;
}

// å–æ¶ˆå¾½ç« å“åº”
message RevokeBadgeResponse {
  bool success = 1;
  string message = 2;
}

// å…‘æ¢å¾½ç« è¯·æ±‚
message RedeemBadgeRequest {
  string user_id = 1;
  string redemption_rule_id = 2;
}

// å…‘æ¢å¾½ç« å“åº”
message RedeemBadgeResponse {
  bool success = 1;
  string order_id = 2;
  string benefit_id = 3;
  string benefit_name = 4;
  string message = 5;
}

// ç½®é¡¶å¾½ç« è¯·æ±‚
message PinBadgeRequest {
  string user_id = 1;
  string user_badge_id = 2;
  bool pin = 3;
}

// ç½®é¡¶å¾½ç« å“åº”
message PinBadgeResponse {
  bool success = 1;
  string message = 2;
}
```

**Step 2: æ›´æ–° build.rs**

```rust
fn main() -> Result<(), Box<dyn std::error::Error>> {
    tonic_build::configure()
        .build_server(true)
        .build_client(true)
        .out_dir("src/generated")
        .compile_protos(
            &[
                "src/rule_engine.proto",
                "src/badge.proto",
            ],
            &["src/"],
        )?;
    Ok(())
}
```

**Step 3: æ›´æ–° lib.rs**

```rust
//! gRPC/Protobuf å®šä¹‰

pub mod rule_engine {
    include!("generated/badge.rule_engine.rs");
}

pub mod badge {
    include!("generated/badge.management.rs");
}
```

**Step 4: ç¼–è¯‘éªŒè¯**

Run: `cargo build -p badge-proto`
Expected: ç¼–è¯‘æˆåŠŸ

**Step 5: æäº¤**

```bash
git add crates/proto/
git commit -m "feat(proto): æ·»åŠ å¾½ç« ç®¡ç†æœåŠ¡ proto å®šä¹‰"
```

---

### Task 2.3: å®ç°å…±äº«é”™è¯¯å¤„ç†æ¨¡å—

**Files:**
- Modify: `crates/shared/src/error.rs`

**Step 1: å®ç°é”™è¯¯ç±»å‹**

```rust
//! ç»Ÿä¸€é”™è¯¯å¤„ç†æ¨¡å—
//!
//! å®šä¹‰ç³»ç»Ÿä¸­æ‰€æœ‰å…±äº«çš„é”™è¯¯ç±»å‹ï¼Œä½¿ç”¨ thiserror æä¾›è‰¯å¥½çš„é”™è¯¯ä¿¡æ¯ã€‚

use thiserror::Error;

/// ç³»ç»Ÿé”™è¯¯ç±»å‹
#[derive(Debug, Error)]
pub enum BadgeError {
    // ==================== æ•°æ®åº“é”™è¯¯ ====================
    #[error("æ•°æ®åº“é”™è¯¯: {0}")]
    Database(#[from] sqlx::Error),

    #[error("è®°å½•æœªæ‰¾åˆ°: {entity} id={id}")]
    NotFound { entity: String, id: String },

    #[error("è®°å½•å·²å­˜åœ¨: {entity} {field}={value}")]
    AlreadyExists {
        entity: String,
        field: String,
        value: String,
    },

    // ==================== ç¼“å­˜é”™è¯¯ ====================
    #[error("Redis é”™è¯¯: {0}")]
    Redis(#[from] redis::RedisError),

    #[error("ç¼“å­˜æœªå‘½ä¸­: {key}")]
    CacheMiss { key: String },

    // ==================== Kafka é”™è¯¯ ====================
    #[error("Kafka é”™è¯¯: {0}")]
    Kafka(String),

    // ==================== ä¸šåŠ¡é€»è¾‘é”™è¯¯ ====================
    #[error("å¾½ç« ä½™é¢ä¸è¶³: éœ€è¦ {required}, å®é™… {actual}")]
    InsufficientBalance { required: i32, actual: i32 },

    #[error("å¾½ç« å·²è¿‡æœŸ: badge_id={badge_id}")]
    BadgeExpired { badge_id: String },

    #[error("å…‘æ¢æ¡ä»¶ä¸æ»¡è¶³: {reason}")]
    RedemptionConditionNotMet { reason: String },

    #[error("æ“ä½œé¢‘ç‡è¶…é™: {operation}")]
    RateLimitExceeded { operation: String },

    #[error("å¾½ç« ä¸å¯ç”¨: {reason}")]
    BadgeUnavailable { reason: String },

    // ==================== è§„åˆ™å¼•æ“é”™è¯¯ ====================
    #[error("è§„åˆ™è§£æå¤±è´¥: {0}")]
    RuleParseFailed(String),

    #[error("è§„åˆ™æ‰§è¡Œå¤±è´¥: {0}")]
    RuleExecutionFailed(String),

    #[error("è§„åˆ™æœªæ‰¾åˆ°: rule_id={rule_id}")]
    RuleNotFound { rule_id: String },

    // ==================== éªŒè¯é”™è¯¯ ====================
    #[error("å‚æ•°éªŒè¯å¤±è´¥: {0}")]
    Validation(String),

    #[error("æ— æ•ˆçš„å‚æ•°: {field} - {message}")]
    InvalidArgument { field: String, message: String },

    // ==================== æƒé™é”™è¯¯ ====================
    #[error("æœªæˆæƒè®¿é—®")]
    Unauthorized,

    #[error("æƒé™ä¸è¶³: {operation}")]
    Forbidden { operation: String },

    // ==================== å¤–éƒ¨æœåŠ¡é”™è¯¯ ====================
    #[error("å¤–éƒ¨æœåŠ¡é”™è¯¯: {service} - {message}")]
    ExternalService { service: String, message: String },

    #[error("å¤–éƒ¨æœåŠ¡è¶…æ—¶: {service}")]
    ExternalServiceTimeout { service: String },

    // ==================== é€šç”¨é”™è¯¯ ====================
    #[error("å†…éƒ¨é”™è¯¯: {0}")]
    Internal(String),

    #[error("{0}")]
    Custom(String),
}

/// é”™è¯¯ç»“æœç±»å‹åˆ«å
pub type Result<T> = std::result::Result<T, BadgeError>;

impl BadgeError {
    /// è·å–é”™è¯¯ç 
    pub fn code(&self) -> &'static str {
        match self {
            Self::Database(_) => "DATABASE_ERROR",
            Self::NotFound { .. } => "NOT_FOUND",
            Self::AlreadyExists { .. } => "ALREADY_EXISTS",
            Self::Redis(_) => "REDIS_ERROR",
            Self::CacheMiss { .. } => "CACHE_MISS",
            Self::Kafka(_) => "KAFKA_ERROR",
            Self::InsufficientBalance { .. } => "INSUFFICIENT_BALANCE",
            Self::BadgeExpired { .. } => "BADGE_EXPIRED",
            Self::RedemptionConditionNotMet { .. } => "REDEMPTION_CONDITION_NOT_MET",
            Self::RateLimitExceeded { .. } => "RATE_LIMIT_EXCEEDED",
            Self::BadgeUnavailable { .. } => "BADGE_UNAVAILABLE",
            Self::RuleParseFailed(_) => "RULE_PARSE_FAILED",
            Self::RuleExecutionFailed(_) => "RULE_EXECUTION_FAILED",
            Self::RuleNotFound { .. } => "RULE_NOT_FOUND",
            Self::Validation(_) => "VALIDATION_ERROR",
            Self::InvalidArgument { .. } => "INVALID_ARGUMENT",
            Self::Unauthorized => "UNAUTHORIZED",
            Self::Forbidden { .. } => "FORBIDDEN",
            Self::ExternalService { .. } => "EXTERNAL_SERVICE_ERROR",
            Self::ExternalServiceTimeout { .. } => "EXTERNAL_SERVICE_TIMEOUT",
            Self::Internal(_) => "INTERNAL_ERROR",
            Self::Custom(_) => "CUSTOM_ERROR",
        }
    }

    /// æ˜¯å¦ä¸ºå¯é‡è¯•é”™è¯¯
    pub fn is_retryable(&self) -> bool {
        matches!(
            self,
            Self::Database(_)
                | Self::Redis(_)
                | Self::Kafka(_)
                | Self::ExternalServiceTimeout { .. }
        )
    }

    /// è½¬æ¢ä¸º gRPC çŠ¶æ€ç 
    pub fn to_grpc_status(&self) -> tonic::Status {
        use tonic::{Code, Status};

        let (code, message) = match self {
            Self::NotFound { .. } => (Code::NotFound, self.to_string()),
            Self::AlreadyExists { .. } => (Code::AlreadyExists, self.to_string()),
            Self::Validation(_) | Self::InvalidArgument { .. } => {
                (Code::InvalidArgument, self.to_string())
            }
            Self::Unauthorized => (Code::Unauthenticated, self.to_string()),
            Self::Forbidden { .. } => (Code::PermissionDenied, self.to_string()),
            Self::RateLimitExceeded { .. } => (Code::ResourceExhausted, self.to_string()),
            Self::ExternalServiceTimeout { .. } => (Code::DeadlineExceeded, self.to_string()),
            _ => (Code::Internal, self.to_string()),
        };

        Status::new(code, message)
    }
}

impl From<BadgeError> for tonic::Status {
    fn from(err: BadgeError) -> Self {
        err.to_grpc_status()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_error_code() {
        let err = BadgeError::NotFound {
            entity: "Badge".to_string(),
            id: "123".to_string(),
        };
        assert_eq!(err.code(), "NOT_FOUND");
    }

    #[test]
    fn test_is_retryable() {
        let db_err = BadgeError::Database(sqlx::Error::PoolTimedOut);
        assert!(db_err.is_retryable());

        let not_found = BadgeError::NotFound {
            entity: "Badge".to_string(),
            id: "123".to_string(),
        };
        assert!(!not_found.is_retryable());
    }
}
```

**Step 2: ç¼–è¯‘å¹¶è¿è¡Œæµ‹è¯•**

Run: `cargo test -p badge-shared error`
Expected: æµ‹è¯•é€šè¿‡

**Step 3: æäº¤**

```bash
git add crates/shared/
git commit -m "feat(shared): å®ç°ç»Ÿä¸€é”™è¯¯å¤„ç†æ¨¡å—"
```

---

### Task 2.4: å®ç°å…±äº«é…ç½®æ¨¡å—

**Files:**
- Modify: `crates/shared/src/config.rs`
- Create: `config/default.toml`

**Step 1: å®ç°é…ç½®æ¨¡å—**

```rust
//! é…ç½®ç®¡ç†æ¨¡å—
//!
//! æ”¯æŒå¤šæ ¼å¼é…ç½®æ–‡ä»¶åŠ è½½ï¼Œç¯å¢ƒå˜é‡è¦†ç›–ï¼Œä»¥åŠç±»å‹å®‰å…¨çš„é…ç½®è®¿é—®ã€‚

use config::{Config, ConfigError, Environment, File};
use serde::Deserialize;
use std::path::Path;

/// æ•°æ®åº“é…ç½®
#[derive(Debug, Clone, Deserialize)]
pub struct DatabaseConfig {
    pub url: String,
    pub max_connections: u32,
    pub min_connections: u32,
    pub connect_timeout_seconds: u64,
    pub idle_timeout_seconds: u64,
}

impl Default for DatabaseConfig {
    fn default() -> Self {
        Self {
            url: "postgres://badge:badge_secret@localhost:5432/badge_db".to_string(),
            max_connections: 10,
            min_connections: 2,
            connect_timeout_seconds: 30,
            idle_timeout_seconds: 600,
        }
    }
}

/// Redis é…ç½®
#[derive(Debug, Clone, Deserialize)]
pub struct RedisConfig {
    pub url: String,
    pub pool_size: u32,
}

impl Default for RedisConfig {
    fn default() -> Self {
        Self {
            url: "redis://localhost:6379".to_string(),
            pool_size: 10,
        }
    }
}

/// Kafka é…ç½®
#[derive(Debug, Clone, Deserialize)]
pub struct KafkaConfig {
    pub brokers: String,
    pub consumer_group: String,
    pub auto_offset_reset: String,
}

impl Default for KafkaConfig {
    fn default() -> Self {
        Self {
            brokers: "localhost:9092".to_string(),
            consumer_group: "badge-service".to_string(),
            auto_offset_reset: "earliest".to_string(),
        }
    }
}

/// æœåŠ¡é…ç½®
#[derive(Debug, Clone, Deserialize)]
pub struct ServerConfig {
    pub host: String,
    pub port: u16,
    pub workers: Option<usize>,
}

impl Default for ServerConfig {
    fn default() -> Self {
        Self {
            host: "0.0.0.0".to_string(),
            port: 8080,
            workers: None,
        }
    }
}

/// å¯è§‚æµ‹æ€§é…ç½®
#[derive(Debug, Clone, Deserialize)]
pub struct ObservabilityConfig {
    pub log_level: String,
    pub log_format: String, // json, pretty
    pub metrics_enabled: bool,
    pub metrics_port: u16,
    pub tracing_enabled: bool,
    pub tracing_endpoint: Option<String>,
}

impl Default for ObservabilityConfig {
    fn default() -> Self {
        Self {
            log_level: "info".to_string(),
            log_format: "pretty".to_string(),
            metrics_enabled: true,
            metrics_port: 9090,
            tracing_enabled: false,
            tracing_endpoint: None,
        }
    }
}

/// åº”ç”¨é…ç½®
#[derive(Debug, Clone, Deserialize, Default)]
pub struct AppConfig {
    pub service_name: String,
    pub environment: String,
    pub server: ServerConfig,
    pub database: DatabaseConfig,
    pub redis: RedisConfig,
    pub kafka: KafkaConfig,
    pub observability: ObservabilityConfig,
}

impl AppConfig {
    /// ä»é…ç½®æ–‡ä»¶å’Œç¯å¢ƒå˜é‡åŠ è½½é…ç½®
    ///
    /// åŠ è½½é¡ºåºï¼š
    /// 1. config/default.tomlï¼ˆé»˜è®¤é…ç½®ï¼‰
    /// 2. config/{environment}.tomlï¼ˆç¯å¢ƒç‰¹å®šé…ç½®ï¼‰
    /// 3. ç¯å¢ƒå˜é‡ï¼ˆBADGE_ å‰ç¼€ï¼‰
    pub fn load(service_name: &str) -> Result<Self, ConfigError> {
        let env = std::env::var("BADGE_ENV").unwrap_or_else(|_| "development".to_string());

        let config_dir = std::env::var("CONFIG_DIR").unwrap_or_else(|_| "config".to_string());

        let builder = Config::builder()
            // é»˜è®¤é…ç½®
            .set_default("service_name", service_name)?
            .set_default("environment", env.clone())?
            // åŠ è½½é»˜è®¤é…ç½®æ–‡ä»¶
            .add_source(File::from(Path::new(&config_dir).join("default.toml")).required(false))
            // åŠ è½½ç¯å¢ƒç‰¹å®šé…ç½®
            .add_source(
                File::from(Path::new(&config_dir).join(format!("{}.toml", env))).required(false),
            )
            // ç¯å¢ƒå˜é‡è¦†ç›–ï¼ˆBADGE_DATABASE_URL -> database.urlï¼‰
            .add_source(
                Environment::with_prefix("BADGE")
                    .separator("_")
                    .try_parsing(true),
            );

        builder.build()?.try_deserialize()
    }

    /// è·å–æœåŠ¡åœ°å€
    pub fn server_addr(&self) -> String {
        format!("{}:{}", self.server.host, self.server.port)
    }

    /// æ˜¯å¦ä¸ºç”Ÿäº§ç¯å¢ƒ
    pub fn is_production(&self) -> bool {
        self.environment == "production"
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_default_config() {
        let config = AppConfig::default();
        assert_eq!(config.server.port, 8080);
        assert_eq!(config.database.max_connections, 10);
    }

    #[test]
    fn test_server_addr() {
        let config = AppConfig {
            server: ServerConfig {
                host: "127.0.0.1".to_string(),
                port: 3000,
                workers: None,
            },
            ..Default::default()
        };
        assert_eq!(config.server_addr(), "127.0.0.1:3000");
    }
}
```

**Step 2: åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶**

`config/default.toml`:
```toml
[server]
host = "0.0.0.0"
port = 8080

[database]
url = "postgres://badge:badge_secret@localhost:5432/badge_db"
max_connections = 10
min_connections = 2
connect_timeout_seconds = 30
idle_timeout_seconds = 600

[redis]
url = "redis://localhost:6379"
pool_size = 10

[kafka]
brokers = "localhost:9092"
consumer_group = "badge-service"
auto_offset_reset = "earliest"

[observability]
log_level = "info"
log_format = "pretty"
metrics_enabled = true
metrics_port = 9090
tracing_enabled = false
```

**Step 3: ç¼–è¯‘å¹¶æµ‹è¯•**

Run: `cargo test -p badge-shared config`
Expected: æµ‹è¯•é€šè¿‡

**Step 4: æäº¤**

```bash
git add crates/shared/ config/
git commit -m "feat(shared): å®ç°é…ç½®ç®¡ç†æ¨¡å—"
```

---

### Task 2.5: å®ç°å…±äº«æ•°æ®åº“è¿æ¥æ¨¡å—

**Files:**
- Modify: `crates/shared/src/database.rs`

**Step 1: å®ç°æ•°æ®åº“æ¨¡å—**

```rust
//! æ•°æ®åº“è¿æ¥ç®¡ç†æ¨¡å—
//!
//! æä¾› PostgreSQL è¿æ¥æ± ç®¡ç†ï¼Œæ”¯æŒå¥åº·æ£€æŸ¥å’Œè¿æ¥é…ç½®ã€‚

use crate::config::DatabaseConfig;
use crate::error::{BadgeError, Result};
use sqlx::postgres::{PgPool, PgPoolOptions};
use std::time::Duration;
use tracing::{info, instrument};

/// æ•°æ®åº“è¿æ¥æ± åŒ…è£…
#[derive(Clone)]
pub struct Database {
    pool: PgPool,
}

impl Database {
    /// åˆ›å»ºæ•°æ®åº“è¿æ¥æ± 
    #[instrument(skip(config))]
    pub async fn connect(config: &DatabaseConfig) -> Result<Self> {
        info!("Connecting to database...");

        let pool = PgPoolOptions::new()
            .max_connections(config.max_connections)
            .min_connections(config.min_connections)
            .acquire_timeout(Duration::from_secs(config.connect_timeout_seconds))
            .idle_timeout(Duration::from_secs(config.idle_timeout_seconds))
            .connect(&config.url)
            .await?;

        info!("Database connection pool created");

        Ok(Self { pool })
    }

    /// è·å–è¿æ¥æ± å¼•ç”¨
    pub fn pool(&self) -> &PgPool {
        &self.pool
    }

    /// å¥åº·æ£€æŸ¥
    pub async fn health_check(&self) -> Result<()> {
        sqlx::query("SELECT 1")
            .execute(&self.pool)
            .await
            .map(|_| ())
            .map_err(BadgeError::from)
    }

    /// å…³é—­è¿æ¥æ± 
    pub async fn close(&self) {
        self.pool.close().await;
        info!("Database connection pool closed");
    }

    /// è¿è¡Œè¿ç§»
    #[instrument(skip(self))]
    pub async fn run_migrations(&self) -> Result<()> {
        info!("Running database migrations...");
        sqlx::migrate!("../../migrations")
            .run(&self.pool)
            .await
            .map_err(|e| BadgeError::Database(e.into()))?;
        info!("Database migrations completed");
        Ok(())
    }
}

impl std::ops::Deref for Database {
    type Target = PgPool;

    fn deref(&self) -> &Self::Target {
        &self.pool
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    #[ignore] // éœ€è¦æ•°æ®åº“è¿æ¥
    async fn test_database_connection() {
        let config = DatabaseConfig::default();
        let db = Database::connect(&config).await.unwrap();
        db.health_check().await.unwrap();
    }
}
```

**Step 2: ç¼–è¯‘éªŒè¯**

Run: `cargo build -p badge-shared`
Expected: ç¼–è¯‘æˆåŠŸ

**Step 3: æäº¤**

```bash
git add crates/shared/
git commit -m "feat(shared): å®ç°æ•°æ®åº“è¿æ¥ç®¡ç†æ¨¡å—"
```

---

### Task 2.6: å®ç°å…±äº« Redis ç¼“å­˜æ¨¡å—

**Files:**
- Modify: `crates/shared/src/cache.rs`

**Step 1: å®ç°ç¼“å­˜æ¨¡å—**

```rust
//! Redis ç¼“å­˜ç®¡ç†æ¨¡å—
//!
//! æä¾› Redis è¿æ¥ç®¡ç†å’Œå¸¸ç”¨ç¼“å­˜æ“ä½œå°è£…ã€‚

use crate::config::RedisConfig;
use crate::error::{BadgeError, Result};
use redis::aio::MultiplexedConnection;
use redis::{AsyncCommands, Client};
use serde::{de::DeserializeOwned, Serialize};
use std::time::Duration;
use tracing::{info, instrument};

/// Redis ç¼“å­˜å®¢æˆ·ç«¯
#[derive(Clone)]
pub struct Cache {
    client: Client,
}

impl Cache {
    /// åˆ›å»º Redis å®¢æˆ·ç«¯
    pub fn new(config: &RedisConfig) -> Result<Self> {
        let client = Client::open(config.url.as_str())?;
        info!("Redis client created");
        Ok(Self { client })
    }

    /// è·å–è¿æ¥
    async fn get_conn(&self) -> Result<MultiplexedConnection> {
        self.client
            .get_multiplexed_async_connection()
            .await
            .map_err(BadgeError::from)
    }

    /// å¥åº·æ£€æŸ¥
    pub async fn health_check(&self) -> Result<()> {
        let mut conn = self.get_conn().await?;
        redis::cmd("PING")
            .query_async::<String>(&mut conn)
            .await
            .map(|_| ())
            .map_err(BadgeError::from)
    }

    /// è·å–å€¼
    #[instrument(skip(self))]
    pub async fn get<T: DeserializeOwned>(&self, key: &str) -> Result<Option<T>> {
        let mut conn = self.get_conn().await?;
        let value: Option<String> = conn.get(key).await?;

        match value {
            Some(v) => {
                let parsed: T = serde_json::from_str(&v)
                    .map_err(|e| BadgeError::Internal(format!("Cache deserialization error: {}", e)))?;
                Ok(Some(parsed))
            }
            None => Ok(None),
        }
    }

    /// è®¾ç½®å€¼
    #[instrument(skip(self, value))]
    pub async fn set<T: Serialize>(&self, key: &str, value: &T, ttl: Duration) -> Result<()> {
        let mut conn = self.get_conn().await?;
        let serialized = serde_json::to_string(value)
            .map_err(|e| BadgeError::Internal(format!("Cache serialization error: {}", e)))?;

        conn.set_ex(key, serialized, ttl.as_secs()).await?;
        Ok(())
    }

    /// åˆ é™¤å€¼
    #[instrument(skip(self))]
    pub async fn delete(&self, key: &str) -> Result<()> {
        let mut conn = self.get_conn().await?;
        conn.del(key).await?;
        Ok(())
    }

    /// æ‰¹é‡åˆ é™¤ï¼ˆæŒ‰æ¨¡å¼ï¼‰
    #[instrument(skip(self))]
    pub async fn delete_pattern(&self, pattern: &str) -> Result<u64> {
        let mut conn = self.get_conn().await?;
        let keys: Vec<String> = conn.keys(pattern).await?;

        if keys.is_empty() {
            return Ok(0);
        }

        let count: u64 = conn.del(keys).await?;
        Ok(count)
    }

    /// æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨
    pub async fn exists(&self, key: &str) -> Result<bool> {
        let mut conn = self.get_conn().await?;
        let exists: bool = conn.exists(key).await?;
        Ok(exists)
    }

    /// è·å–æˆ–è®¾ç½®ï¼ˆç¼“å­˜ç©¿é€ä¿æŠ¤ï¼‰
    #[instrument(skip(self, loader))]
    pub async fn get_or_set<T, F, Fut>(
        &self,
        key: &str,
        ttl: Duration,
        loader: F,
    ) -> Result<T>
    where
        T: Serialize + DeserializeOwned,
        F: FnOnce() -> Fut,
        Fut: std::future::Future<Output = Result<T>>,
    {
        // å°è¯•ä»ç¼“å­˜è·å–
        if let Some(cached) = self.get::<T>(key).await? {
            return Ok(cached);
        }

        // ä»æ•°æ®æºåŠ è½½
        let value = loader().await?;

        // å†™å…¥ç¼“å­˜
        self.set(key, &value, ttl).await?;

        Ok(value)
    }

    /// å¢é‡æ“ä½œ
    pub async fn incr(&self, key: &str, delta: i64) -> Result<i64> {
        let mut conn = self.get_conn().await?;
        let result: i64 = conn.incr(key, delta).await?;
        Ok(result)
    }

    /// è®¾ç½®è¿‡æœŸæ—¶é—´
    pub async fn expire(&self, key: &str, ttl: Duration) -> Result<()> {
        let mut conn = self.get_conn().await?;
        conn.expire(key, ttl.as_secs() as i64).await?;
        Ok(())
    }
}

/// ç¼“å­˜é”®ç”Ÿæˆå™¨
pub struct CacheKey;

impl CacheKey {
    pub fn user_badges(user_id: &str) -> String {
        format!("user:badge:{}", user_id)
    }

    pub fn badge_detail(badge_id: &str) -> String {
        format!("badge:detail:{}", badge_id)
    }

    pub fn badge_config(badge_id: &str) -> String {
        format!("badge:config:{}", badge_id)
    }

    pub fn user_badge_count(user_id: &str) -> String {
        format!("user:badge:count:{}", user_id)
    }

    pub fn rule(rule_id: &str) -> String {
        format!("rule:{}", rule_id)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_cache_key_generation() {
        assert_eq!(CacheKey::user_badges("123"), "user:badge:123");
        assert_eq!(CacheKey::badge_detail("abc"), "badge:detail:abc");
    }
}
```

**Step 2: ç¼–è¯‘éªŒè¯**

Run: `cargo build -p badge-shared`
Expected: ç¼–è¯‘æˆåŠŸ

**Step 3: æäº¤**

```bash
git add crates/shared/
git commit -m "feat(shared): å®ç° Redis ç¼“å­˜ç®¡ç†æ¨¡å—"
```

---

## Phase 3-9: åç»­é˜¶æ®µ

ç”±äºç¯‡å¹…é™åˆ¶ï¼Œåç»­é˜¶æ®µï¼ˆPhase 3-9ï¼‰å°†åœ¨å®ç°è¿‡ç¨‹ä¸­é€æ­¥å±•å¼€ã€‚æ¯ä¸ªé˜¶æ®µåŒ…å«ï¼š

### Phase 3: è§„åˆ™å¼•æ“æœåŠ¡ (10 tasks)
- è§„åˆ™ JSON è§£æå™¨
- è§„åˆ™ç¼–è¯‘å™¨ï¼ˆAST æ„å»ºï¼‰
- è§„åˆ™æ‰§è¡Œå™¨ï¼ˆçŸ­è·¯æ±‚å€¼ï¼‰
- è§„åˆ™ç¼“å­˜ä¸çƒ­æ›´æ–°
- gRPC æœåŠ¡å®ç°
- å•å…ƒæµ‹è¯•ä¸é›†æˆæµ‹è¯•

### Phase 4: å¾½ç« ç®¡ç†æœåŠ¡ Cç«¯ (12 tasks)
- å¾½ç« æŸ¥è¯¢æœåŠ¡
- å¾½ç« å¢™æœåŠ¡
- å¾½ç« å‘æ”¾æœåŠ¡
- å¾½ç« å–æ¶ˆæœåŠ¡
- å…‘æ¢æœåŠ¡ï¼ˆå«äº‹åŠ¡ï¼‰
- è´¦æœ¬è®°å½•æœåŠ¡
- gRPC æœåŠ¡å®ç°

### Phase 5: å¾½ç« ç®¡ç†æœåŠ¡ Bç«¯ (10 tasks)
- å¾½ç«  CRUD API
- è§„åˆ™é…ç½® API
- å‘æ”¾ç®¡ç† API
- ç»Ÿè®¡æŠ¥è¡¨ API
- æ‰¹é‡å¯¼å…¥æœåŠ¡
- ç³»ç»Ÿç®¡ç† API

### Phase 6: äº‹ä»¶å¤„ç†æœåŠ¡ (8 tasks)
- Kafka æ¶ˆè´¹è€…å®ç°
- äº‹ä»¶å¤„ç†ç®¡é“
- è§„åˆ™åŒ¹é…ä¸å¾½ç« å‘æ”¾
- å¹‚ç­‰å¤„ç†ä¸å»é‡
- æ­»ä¿¡é˜Ÿåˆ—å¤„ç†

### Phase 7: æ¨¡æ‹Ÿå¤–éƒ¨ç³»ç»Ÿ (8 tasks)
- Mock è®¢å•æœåŠ¡
- Mock Profile æœåŠ¡
- Mock Coupon æœåŠ¡
- Mock äº‹ä»¶ç”Ÿæˆå™¨
- åœºæ™¯æ¨¡æ‹Ÿå™¨

### Phase 8: ç®¡ç†åå°å‰ç«¯ (15 tasks)
- å¸ƒå±€ä¸è·¯ç”±
- å¾½ç« ç®¡ç†é¡µé¢
- è§„åˆ™ç”»å¸ƒç»„ä»¶
- å‘æ”¾ç®¡ç†é¡µé¢
- æ•°æ®çœ‹æ¿
- ä¼šå‘˜è§†å›¾

### Phase 9: é›†æˆæµ‹è¯•ä¸ä¼˜åŒ– (5 tasks)
- ç«¯åˆ°ç«¯æµ‹è¯•
- æ€§èƒ½æµ‹è¯•
- å®‰å…¨å®¡è®¡
- æ–‡æ¡£å®Œå–„

---

## æ‰§è¡Œæ£€æŸ¥ç‚¹

æ¯ä¸ª Phase å®Œæˆåéœ€è¦éªŒè¯ï¼š

1. **ç¼–è¯‘é€šè¿‡**: `cargo build --workspace`
2. **æµ‹è¯•é€šè¿‡**: `cargo test --workspace`
3. **Lint é€šè¿‡**: `cargo clippy --workspace -- -D warnings`
4. **æäº¤ä»£ç **: åˆ›å»º milestone commit

---

## ä¸‹ä¸€æ­¥

è®¡åˆ’å·²ä¿å­˜è‡³ `docs/plans/2025-01-28-badge-impl-plan.md`ã€‚

ä¸¤ç§æ‰§è¡Œæ–¹å¼ï¼š

**1. Subagent-Drivenï¼ˆå½“å‰ä¼šè¯ï¼‰** - æ¯ä¸ªä»»åŠ¡æ´¾å‘æ–° subagentï¼Œä»»åŠ¡é—´è¿›è¡Œä»£ç å®¡æŸ¥ï¼Œå¿«é€Ÿè¿­ä»£

**2. Parallel Sessionï¼ˆç‹¬ç«‹ä¼šè¯ï¼‰** - åœ¨æ–°ä¼šè¯ä¸­ä½¿ç”¨ executing-plansï¼Œæ‰¹é‡æ‰§è¡Œå¸¦æ£€æŸ¥ç‚¹

ä½ æ›´å€¾å‘äºå“ªç§æ–¹å¼ï¼Ÿ
